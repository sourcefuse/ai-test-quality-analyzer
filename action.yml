name: "Check Unit Test Cases Quality"
description: "Analyze unit test quality against JIRA requirements and generate comprehensive reports"
author: "Vishal Gupta"

branding:
  icon: "check-circle"
  color: "green"

inputs:
  database_host:
    description: "PostgreSQL database host"
    required: false
    default: "localhost"
  database_port:
    description: "PostgreSQL database port"
    required: false
    default: "5432"
  database_name:
    description: "PostgreSQL database name"
    required: false
    default: "postgres-pgvector"
  database_user:
    description: "PostgreSQL database user"
    required: false
    default: "postgres"
  database_password:
    description: "PostgreSQL database password"
    required: false
    default: "admin"
  jira_url:
    description: "JIRA instance URL"
    required: true
  jira_email:
    description: "JIRA user email"
    required: true
  jira_api_token:
    description: "JIRA API token"
    required: true
  jira_ticket_id:
    description: "JIRA ticket ID to analyze (e.g., BB-15690)"
    required: true
  jira_project_key:
    description: "JIRA project key"
    required: true
  confluence_url:
    description: "Confluence instance URL"
    required: true
  confluence_email:
    description: "Confluence user email"
    required: true
  confluence_api_token:
    description: "Confluence API token"
    required: true
  confluence_space_key:
    description: "Confluence space key"
    required: true
  confluence_silent_mode:
    description: "Suppress logging during Confluence page downloads (true/false)"
    required: false
    default: "true"
  current_analysis_path:
    description: "Specific analysis folder to use (optional, auto-generated if not provided)"
    required: false
    default: ""
  repository_url:
    description: "Repository URL to clone for test analysis"
    required: true
  repository_branch:
    description: "Branch name to clone"
    required: false
    default: "main"
  save_to_file:
    description: "Save analysis results to files (true/false)"
    required: false
    default: "true"
  upload_to_confluence:
    description: "Upload results to Confluence (true/false)"
    required: false
    default: "true"
  minimum_score:
    description: "Minimum acceptable test quality score (0-10)"
    required: false
    default: "6.0"
  aws_region_bedrock:
    description: "AWS region for Bedrock service"
    required: false
    default: "us-east-2"
  aws_access_key_bedrock:
    description: "AWS access key for Bedrock authentication"
    required: true
  aws_secret_key_bedrock:
    description: "AWS secret key for Bedrock authentication"
    required: true
  aws_bedrock_model:
    description: "AWS Bedrock model identifier"
    required: false
    default: "us.anthropic.claude-3-5-sonnet-20241022-v2:0"
  anthropic_model:
    description: "Anthropic model name for Claude"
    required: false
    default: "sonnet[1m]"
  ai_type:
    description: "AI provider type: 1 = claude_with_bedrock, 2 = claude_with_glm"
    required: false
    default: "1"
  anthropic_base_url:
    description: "Anthropic/GLM base URL (required when ai_type=2)"
    required: false
  anthropic_auth_token:
    description: "Anthropic/GLM auth token (required when ai_type=2)"
    required: false
  base_folder_suffix:
    description: "Base folder suffix for organizing analysis results"
    required: false
    default: "Quality-Check-Via-AI"
  confluence_root_page_suffix:
    description: "Confluence root page suffix for organizing reports"
    required: false
    default: "Quality-Check-Via-AI"
  confluence_upload_url:
    description: "Confluence instance URL for uploading results (if different from fetch URL)"
    required: false
    default: ""
  confluence_upload_email:
    description: "Confluence user email for uploading results (if different from fetch email)"
    required: false
    default: ""
  confluence_upload_api_token:
    description: "Confluence API token for uploading results (if different from fetch token)"
    required: false
    default: ""
  confluence_upload_space_key:
    description: "Confluence space key for uploading results (if different from fetch space)"
    required: false
    default: ""
  openai_api_key:
    description: "OpenAI API key for embeddings (required if USE_POSTGRES_VECTOR_DB is enabled)"
    required: false
    default: ""
  use_postgres_vector_db:
    description: "Enable PostgreSQL Vector Database with pgvector for RAG-based Confluence fetching (true/false)"
    required: false
    default: "true"
  enable_docker_services:
    description: "Enable Docker services (pgvector and Presidio from ghcr.io) - true/false"
    required: false
    default: "true"
  embedding_dimensions:
    description: "Embedding vector dimensions (1536 for OpenAI, 768 for Ollama nomic-embed-text)"
    required: false
    default: "1536"

outputs:
  test_quality_score:
    description: "Test quality score (0-10)"
    value: ${{ steps.extract_score.outputs.score }}
  analysis_path:
    description: "Path to the analysis results folder"
    value: ${{ steps.extract_paths.outputs.analysis_path }}
  requirements_file:
    description: "Path to the requirements file"
    value: ${{ steps.extract_paths.outputs.requirements_file }}
  result_file:
    description: "Path to the result file"
    value: ${{ steps.extract_paths.outputs.result_file }}
  score_passed:
    description: "Whether the score meets the minimum threshold (true/false)"
    value: ${{ steps.check_score.outputs.passed }}
  confluence_url:
    description: "Confluence URL to the uploaded analysis report"
    value: ${{ steps.upload_confluence.outputs.confluence_url }}

runs:
  using: "composite"
  steps:
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"

    - name: Install dependencies
      shell: bash
      run: |
        cd ${{ github.action_path }}
        npm install
        npm install -g @anthropic-ai/claude-code

    - name: Debug - Print Configuration
      shell: bash
      run: |
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo "â•‘           ğŸ”§ CONFIGURATION DEBUG INFO                        â•‘"
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ“‹ JIRA Configuration:"
        echo "   JIRA Ticket ID: ${{ inputs.jira_ticket_id }}"
        echo "   JIRA Project Key: ${{ inputs.jira_project_key }}"
        echo ""
        echo "ğŸ“„ Confluence Configuration:"
        echo "   Confluence Space Key: ${{ inputs.confluence_space_key }}"
        echo ""
        echo "ğŸ¤– AI Configuration:"
        echo "   AI Type: ${{ inputs.ai_type }} (1=Bedrock, 2=GLM)"
        if [ "${{ inputs.ai_type }}" == "2" ]; then
          echo "   âœ… Using GLM (Anthropic API)"
          echo "   Anthropic Base URL: ${{ inputs.anthropic_base_url && '***configured***' || 'not set' }}"
          echo "   Anthropic Model: ${{ inputs.anthropic_model || 'default' }}"
        else
          echo "   âœ… Using AWS Bedrock"
          echo "   AWS Region: ${{ inputs.aws_region_bedrock || 'not set' }}"
          echo "   Bedrock Model: ${{ inputs.aws_bedrock_model || 'default' }}"
        fi
        echo ""
        echo "ğŸ˜ PostgreSQL Configuration:"
        echo "   Use PostgreSQL Vector DB: ${{ inputs.use_postgres_vector_db }}"
        echo "   Database Host: ${{ inputs.database_host }}"
        echo "   Database Name: ${{ inputs.database_name }}"
        echo "   Embedding Dimensions: ${{ inputs.embedding_dimensions }}"
        echo ""
        echo "ğŸ³ Docker Configuration:"
        echo "   Enable Docker Services: ${{ inputs.enable_docker_services }}"
        echo ""
        echo "ğŸ“Š Analysis Configuration:"
        echo "   Minimum Score: ${{ inputs.minimum_score }}"
        echo "   Upload to Confluence: ${{ inputs.upload_to_confluence }}"
        echo "   Save to File: ${{ inputs.save_to_file }}"
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: Login to GitHub Container Registry
      if: inputs.enable_docker_services == 'true'
      shell: bash
      env:
        GITHUB_TOKEN: ${{ github.token }}
        # ğŸ”’ Security: Use env var for github.actor to prevent command injection
        GITHUB_ACTOR: ${{ github.actor }}
      run: |
        echo "ğŸ³ Logging in to GitHub Container Registry..."
        echo "$GITHUB_TOKEN" | docker login ghcr.io -u "$GITHUB_ACTOR" --password-stdin

    - name: Setup PostgreSQL Database
      if: inputs.database_host == 'localhost' && inputs.enable_docker_services == 'true'
      shell: bash
      env:
        DB_USER: ${{ inputs.database_user }}
        DB_PASSWORD: ${{ inputs.database_password }}
        DB_NAME: ${{ inputs.database_name }}
        DB_PORT: ${{ inputs.database_port }}
      run: |
        echo "ğŸ˜ Setting up PostgreSQL database..."

        # Pull PostgreSQL + pgvector image from ghcr.io (quiet mode)
        echo "ğŸ“¥ Pulling PostgreSQL + pgvector image from ghcr.io..."
        docker pull -q ghcr.io/sourcefuse/pgvector-rag:latest

        # Start PostgreSQL container
        echo "ğŸš€ Starting PostgreSQL container..."
        docker run -d \
          --name postgres-pgvector \
          -e POSTGRES_USER="$DB_USER" \
          -e POSTGRES_PASSWORD="$DB_PASSWORD" \
          -e POSTGRES_DB="$DB_NAME" \
          -p "${DB_PORT}:5432" \
          ghcr.io/sourcefuse/pgvector-rag:latest

        # Wait for PostgreSQL to be ready
        echo "â³ Waiting for PostgreSQL to be ready..."
        sleep 10

        # Verify PostgreSQL is running
        echo "âœ… PostgreSQL setup complete"
        docker ps | grep postgres-pgvector

    - name: Start Presidio services
      if: inputs.enable_docker_services == 'true'
      shell: bash
      run: |
        echo "ğŸ“¥ Pulling Presidio images from ghcr.io (quiet mode)..."
        docker pull -q ghcr.io/sourcefuse/presidio-analyzer:latest &
        docker pull -q ghcr.io/sourcefuse/presidio-anonymizer:latest &
        wait
        echo "âœ… Presidio images pulled successfully"

        echo "ğŸš€ Starting Presidio Analyzer (port 5002)..."
        docker run -d \
          --name presidio-analyzer \
          -p 5002:3000 \
          -e PORT=3000 \
          ghcr.io/sourcefuse/presidio-analyzer:latest

        echo "ğŸš€ Starting Presidio Anonymizer (port 5001)..."
        docker run -d \
          --name presidio-anonymizer \
          -p 5001:3000 \
          -e PORT=3000 \
          ghcr.io/sourcefuse/presidio-anonymizer:latest

        echo "â³ Waiting for services to be ready..."
        sleep 15

        echo "ğŸ” Testing Presidio services availability..."

        # Wait up to 120 seconds for Presidio Analyzer to be ready (spaCy model loading takes time)
        MAX_WAIT=120
        ELAPSED=0
        ANALYZER_READY=false

        while [ $ELAPSED -lt $MAX_WAIT ]; do
          if curl -s http://localhost:5002/health --max-time 5 | grep -q "Presidio"; then
            ANALYZER_READY=true
            echo "âœ… Presidio Analyzer is ready (took ${ELAPSED}s)"
            break
          fi
          echo "   â³ Waiting for Presidio Analyzer... (${ELAPSED}s/${MAX_WAIT}s)"
          sleep 5
          ELAPSED=$((ELAPSED + 5))
        done

        if [ "$ANALYZER_READY" = false ]; then
          echo "âš ï¸  Presidio Analyzer did not respond within ${MAX_WAIT}s - PII detection will use regex fallback"
        fi

        # Wait up to 60 seconds for Presidio Anonymizer to be ready (no model loading needed)
        MAX_WAIT=60
        ELAPSED=0
        ANONYMIZER_READY=false

        while [ $ELAPSED -lt $MAX_WAIT ]; do
          if curl -s http://localhost:5001/health --max-time 5 | grep -q "Presidio"; then
            ANONYMIZER_READY=true
            echo "âœ… Presidio Anonymizer is ready (took ${ELAPSED}s)"
            break
          fi
          echo "   â³ Waiting for Presidio Anonymizer... (${ELAPSED}s/${MAX_WAIT}s)"
          sleep 5
          ELAPSED=$((ELAPSED + 5))
        done

        if [ "$ANONYMIZER_READY" = false ]; then
          echo "âš ï¸  Presidio Anonymizer did not respond within ${MAX_WAIT}s - PII detection will use regex fallback"
        fi

        if [ "$ANALYZER_READY" = true ] && [ "$ANONYMIZER_READY" = true ]; then
          echo "âœ… All Presidio services are ready and responding"
        else
          echo "âš ï¸  Some Presidio services are not ready - application will use regex-based PII detection"
        fi

        echo ""
        echo "ğŸ“Š Presidio container status:"
        docker ps | grep presidio

    - name: Skip Docker services notification
      if: inputs.enable_docker_services != 'true'
      shell: bash
      run: |
        echo "âš ï¸  Skipping Docker services setup"
        echo "   enable_docker_services is not set to 'true'"
        echo "   PII detection will use regex-based fallback"

    - name: Clone repository for analysis
      shell: bash
      env:
        # ğŸ”’ Security: Use env vars to prevent command injection
        # Never use ${{ }} directly in bash scripts with untrusted input
        BRANCH_NAME: ${{ github.head_ref || github.ref_name }}
        GITHUB_TOKEN: ${{ github.token }}
        REPO_NAME: ${{ github.repository }}
        ACTION_PATH: ${{ github.action_path }}
      run: |
        cd "$ACTION_PATH"
        echo "ğŸ“¥ Cloning repository for test analysis..."

        # âœ… SAFE: Sanitize branch name - only allow alphanumeric, hyphen, underscore, slash, dot
        BRANCH=$(echo "$BRANCH_NAME" | tr -cd 'A-Za-z0-9._/-')
        echo "Branch to clone: $BRANCH"

        mkdir -p repo
        cd repo
        git clone --depth 1 --branch "$BRANCH" --single-branch "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO_NAME}.git" .
        echo "âœ… Repository cloned successfully (branch: $BRANCH)"

    - name: Create .env file
      shell: bash
      run: |
        cd ${{ github.action_path }}
        cat > .env << EOF
        # Application Configuration
        NODE_ENV=production

        # JIRA Configuration
        JIRA_URL=${{ inputs.jira_url }}
        JIRA_EMAIL=${{ inputs.jira_email }}
        JIRA_API_TOKEN=${{ inputs.jira_api_token }}
        JIRA_TICKET_ID=${{ inputs.jira_ticket_id }}
        JIRA_PROJECT_KEY=${{ inputs.jira_project_key }}
        JIRA_FETCH_FIELDS=summary,description,customfield_10000
        JIRA_MAX_RESULT=100
        JIRA_FILE_NAME=Jira.md

        # Confluence Configuration (for fetching data)
        CONFLUENCE_URL=${{ inputs.confluence_url }}
        CONFLUENCE_EMAIL=${{ inputs.confluence_email }}
        CONFLUENCE_API_TOKEN=${{ inputs.confluence_api_token }}
        CONFLUENCE_SPACE_KEY=${{ inputs.confluence_space_key }}
        CONFLUENCE_SILENT_MODE=${{ inputs.confluence_silent_mode }}
        CONFLUENCE_MAX_PAGES=
        CONFLUENCE_PAGE_LIMIT=50
        CONFLUENCE_FILE_NAME=Confluence.md
        CONFLUENCE_RAG_FILE_NAME=Confluence-Rag.md

        # Confluence Upload Configuration (for uploading results)
        CONFLUENCE_UPLOAD_URL=${{ inputs.confluence_upload_url }}
        CONFLUENCE_UPLOAD_EMAIL=${{ inputs.confluence_upload_email }}
        CONFLUENCE_UPLOAD_API_TOKEN=${{ inputs.confluence_upload_api_token }}
        CONFLUENCE_UPLOAD_SPACE_KEY=${{ inputs.confluence_upload_space_key }}

        # Presidio Configuration (PII Detection)
        PRESIDIO_ANALYZE_URL=http://localhost:5002/analyze
        PRESIDIO_ANONYMIZE_URL=http://localhost:5001/anonymize
        PRESIDIO_TIMEOUT=30000

        # Data Sanitization Configuration
        SANITIZE_PG_DATA=false

        # AI Provider Configuration
        AI_TYPE=${{ inputs.ai_type }}
        ANTHROPIC_BASE_URL=${{ inputs.anthropic_base_url }}
        ANTHROPIC_AUTH_TOKEN=${{ inputs.anthropic_auth_token }}

        # AWS Bedrock Configuration
        AWS_REGION_BEDROCK=${{ inputs.aws_region_bedrock }}
        AWS_ACCESS_KEY_BEDROCK=${{ inputs.aws_access_key_bedrock }}
        AWS_SECRET_KEY_BEDROCK=${{ inputs.aws_secret_key_bedrock }}
        AWS_BEDROCK_MODEL=${{ inputs.aws_bedrock_model }}
        ANTHROPIC_MODEL=${{ inputs.anthropic_model }}
        BASE_FOLDER_SUFFIX=${{ inputs.base_folder_suffix }}
        CONFLUENCE_ROOT_PAGE_SUFFIX=${{ inputs.confluence_root_page_suffix }}

        # Analysis Configuration
        CURRENT_ANALYSIS_PATH=$(echo $(date +'%Y%m%d-%H-%M-%S')-Via-AI)
        SAVE_TO_FILE=${{ inputs.save_to_file }}

        # File Names Configuration
        REQUIREMENTS_FILE_NAME=Requirements.md
        ANALYSIS_REPORT_FILE_NAME=AnalysisReport.md

        # RAG System Configuration (PostgreSQL + pgvector)
        USE_POSTGRES_VECTOR_DB=${{ inputs.use_postgres_vector_db }}
        DATABASE_HOST=${{ inputs.database_host }}
        DATABASE_PORT=${{ inputs.database_port }}
        DATABASE_NAME=${{ inputs.database_name }}
        DATABASE_USER=${{ inputs.database_user }}
        DATABASE_PASSWORD=${{ inputs.database_password }}

        # OpenAI Embeddings Configuration
        EMBEDDING_PROVIDER=openai
        OPENAI_API_KEY=${{ inputs.openai_api_key }}

        # Performance Configuration
        EMBEDDING_CONCURRENCY=50
        INDEXER_BATCH_SIZE=50
        CHUNK_SIZE=1000
        CHUNK_OVERLAP=200

        # Smart Filter Configuration
        USE_SMART_FILTER=true
        SMART_FILTER_MAX_PAGES=30
        SMART_FILTER_MIN_SCORE=0.3
        SMART_FILTER_USE_KEYWORDS=true
        SMART_FILTER_USE_TITLE=true
        SMART_FILTER_USE_LABELS=true
        SMART_FILTER_USE_COMPONENTS=true
        SMART_FILTER_DEBUG=false
        CHECK_PG_BEFORE_CONFLUENCE_FETCH=true

        EOF
        echo "âœ… Environment configuration created"

    - name: Test PostgreSQL Database Connection
      if: inputs.database_host == 'localhost' && inputs.enable_docker_services == 'true'
      shell: bash
      env:
        DB_USER: ${{ inputs.database_user }}
        DB_NAME: ${{ inputs.database_name }}
      run: |
        echo "ğŸ” Testing PostgreSQL database connection..."

        # Wait a bit more for PostgreSQL to fully initialize
        echo "â³ Waiting for PostgreSQL to fully initialize..."
        sleep 5

        # Test connection and run sample queries
        echo "ğŸ“Š Running sample queries..."
        docker exec postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" -c "SELECT version();"

        echo ""
        echo "ğŸ“¦ Checking pgvector extension..."
        docker exec postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" -c "CREATE EXTENSION IF NOT EXISTS vector;"
        docker exec postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" -c "SELECT * FROM pg_extension WHERE extname = 'vector';"

        echo ""
        echo "âœ… Database connection test complete!"

    - name: Get cache date key
      id: cache-date
      shell: bash
      run: |
        CACHE_DATE=$(date +'%Y-%m-%d')
        echo "date=$CACHE_DATE" >> $GITHUB_OUTPUT
        echo "ğŸ“… Cache date key: $CACHE_DATE"

    - name: Debug - Cache Configuration
      if: inputs.use_postgres_vector_db == 'true' && inputs.database_host == 'localhost'
      shell: bash
      run: |
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo "â•‘           ğŸ’¾ CACHE DEBUG INFO                                â•‘"
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ”‘ Cache Key: pgvector-${{ inputs.confluence_space_key }}-dim${{ inputs.embedding_dimensions }}-${{ steps.cache-date.outputs.date }}"
        echo "ğŸ“‚ Cache Path: /tmp/pgvector-data"
        echo "â„¹ï¸  Cache Policy: Daily rotation (no fallback to older cache)"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: Restore PostgreSQL data from cache
      if: inputs.use_postgres_vector_db == 'true' && inputs.database_host == 'localhost'
      id: pgdata-cache
      uses: actions/cache@v4
      with:
        path: /tmp/pgvector-data
        key: pgvector-${{ inputs.confluence_space_key }}-dim${{ inputs.embedding_dimensions }}-${{ steps.cache-date.outputs.date }}

    - name: Debug - Cache Restore Result
      if: inputs.use_postgres_vector_db == 'true' && inputs.database_host == 'localhost'
      shell: bash
      run: |
        echo "ğŸ“¦ Cache Restore Result:"
        echo "   Cache Hit: ${{ steps.pgdata-cache.outputs.cache-hit || 'false' }}"
        if [ -f /tmp/pgvector-data/pgdump.sql ]; then
          CACHE_SIZE=$(ls -lh /tmp/pgvector-data/pgdump.sql | awk '{print $5}')
          echo "   Cache File Found: âœ… Yes"
          echo "   Cache File Size: $CACHE_SIZE"
        else
          echo "   Cache File Found: âŒ No"
        fi

    - name: Load PostgreSQL data from cache
      if: inputs.database_host == 'localhost'
      id: load-cache
      shell: bash
      env:
        DB_USER: ${{ inputs.database_user }}
        DB_NAME: ${{ inputs.database_name }}
      run: |
        echo "ğŸ“¦ Checking for cached PostgreSQL data..."
        if [ -f /tmp/pgvector-data/pgdump.sql ]; then
          # Truncate existing tables before restore (schema already exists from init scripts)
          echo "ğŸ—‘ï¸  Clearing existing data..."
          docker exec postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" -c "TRUNCATE TABLE document_chunks, confluence_documents CASCADE;" 2>/dev/null || true

          # Restore the data-only dump (ignore errors for schema objects)
          echo "ğŸ“¥ Restoring data from cache..."
          docker exec -i postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" --set ON_ERROR_STOP=off < /tmp/pgvector-data/pgdump.sql 2>/dev/null || true

          # Verify restoration
          DOC_COUNT=$(docker exec postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT COUNT(*) FROM confluence_documents;" 2>/dev/null || echo "0")
          DOC_COUNT=$(echo "$DOC_COUNT" | tr -d ' ')

          CHUNK_COUNT=$(docker exec postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT COUNT(*) FROM document_chunks;" 2>/dev/null || echo "0")
          CHUNK_COUNT=$(echo "$CHUNK_COUNT" | tr -d ' ')

          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘           ğŸ“Š CACHE LOAD RESULT                               â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "   Documents Restored: $DOC_COUNT"
          echo "   Chunks Restored: $CHUNK_COUNT"

          if [ "$DOC_COUNT" -gt 0 ]; then
            echo "   Status: âœ… CACHE HIT - Data loaded successfully"
            echo "   Confluence fetch will be SKIPPED!"
            echo "cache_loaded=true" >> $GITHUB_OUTPUT
          else
            echo "   Status: âš ï¸ CACHE MISS - No documents found after restore"
            echo "   Confluence fetch will be REQUIRED"
            echo "cache_loaded=false" >> $GITHUB_OUTPUT
          fi
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        else
          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘           ğŸ“Š CACHE LOAD RESULT                               â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "   Status: âš ï¸ CACHE MISS - No cache file found"
          echo "   Confluence fetch will be REQUIRED"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "cache_loaded=false" >> $GITHUB_OUTPUT
        fi

    - name: Step 1 - Fetch JIRA and Confluence data
      shell: bash
      env:
        ACTION_PATH: ${{ github.action_path }}
        SKIP_CONFLUENCE_FETCH: ${{ steps.load-cache.outputs.cache_loaded }}
      run: |
        cd "$ACTION_PATH"
        echo "ğŸ”„ Fetching JIRA ticket and Confluence pages..."
        if [ "$SKIP_CONFLUENCE_FETCH" == "true" ]; then
          echo "   â„¹ï¸  SKIP_CONFLUENCE_FETCH=true (cache loaded)"
        fi
        npm run start
        echo "âœ… Data fetched successfully"

    - name: Step 2 - Fetch Confluence with RAG
      if: inputs.use_postgres_vector_db == 'true' && steps.load-cache.outputs.cache_loaded != 'true'
      shell: bash
      env:
        ACTION_PATH: ${{ github.action_path }}
      run: |
        cd "$ACTION_PATH"
        echo "ğŸ”„ Fetching Confluence data using RAG-based approach..."
        npm run process:jira
        echo "âœ… Confluence RAG data fetched successfully"

    - name: Skip Confluence RAG fetch (cache hit)
      if: inputs.use_postgres_vector_db == 'true' && steps.load-cache.outputs.cache_loaded == 'true'
      shell: bash
      run: |
        echo "âœ… Skipping Confluence RAG fetch - data loaded from cache"

    - name: Save PostgreSQL data to cache
      if: inputs.use_postgres_vector_db == 'true' && inputs.database_host == 'localhost' && steps.load-cache.outputs.cache_loaded != 'true'
      shell: bash
      env:
        DB_USER: ${{ inputs.database_user }}
        DB_NAME: ${{ inputs.database_name }}
      run: |
        echo "ğŸ’¾ Saving PostgreSQL data to cache..."
        mkdir -p /tmp/pgvector-data

        # Check if table exists and has data
        TABLE_EXISTS=$(docker exec postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'confluence_documents');" 2>/dev/null || echo "f")
        TABLE_EXISTS=$(echo "$TABLE_EXISTS" | tr -d ' ')

        if [ "$TABLE_EXISTS" = "t" ]; then
          DOC_COUNT=$(docker exec postgres-pgvector psql -U "$DB_USER" -d "$DB_NAME" -t -c "SELECT COUNT(*) FROM confluence_documents;" 2>/dev/null || echo "0")
          DOC_COUNT=$(echo "$DOC_COUNT" | tr -d ' ')

          if [ "$DOC_COUNT" -gt 0 ]; then
            echo "ğŸ“Š Found $DOC_COUNT documents to cache"

            # Export data only (schema is created by Docker init scripts)
            docker exec postgres-pgvector pg_dump -U "$DB_USER" -d "$DB_NAME" --data-only --no-owner --no-acl > /tmp/pgvector-data/pgdump.sql

            DUMP_SIZE=$(du -h /tmp/pgvector-data/pgdump.sql | cut -f1)
            echo "âœ… PostgreSQL dump created: $DUMP_SIZE"
            echo "   Cache will be saved for next run"
          else
            echo "âš ï¸  No documents found, skipping cache save"
          fi
        else
          echo "âš ï¸  Table confluence_documents not found, skipping cache save"
        fi

    - name: Step 3 - Create Requirements document (GLM)
      if: inputs.ai_type == '2'
      shell: bash
      env:
        ANTHROPIC_BASE_URL: ${{ inputs.anthropic_base_url }}
        ANTHROPIC_AUTH_TOKEN: ${{ inputs.anthropic_auth_token }}
        ANTHROPIC_MODEL: ${{ inputs.anthropic_model }}
      run: |
        cd ${{ github.action_path }}
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo "â•‘           ğŸ¤– AI ANALYSIS - GLM (Anthropic API)               â•‘"
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ“‹ Creating requirements document from JIRA and Confluence..."
        echo "   AI Provider: GLM (Anthropic API)"
        echo "   Base URL: ${ANTHROPIC_BASE_URL:-not set}"
        echo "   Model: ${ANTHROPIC_MODEL:-default}"
        echo ""
        npm run create-requirement-doc
        echo "âœ… Requirements document created"
        echo ""
        echo "ğŸ” Analyzing unit test quality..."
        npm run analyze-test-quality
        echo "âœ… Test quality analysis completed"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: Step 3 - Create Requirements document (Bedrock)
      if: inputs.ai_type == '1' || inputs.ai_type == ''
      shell: bash
      env:
        CLAUDE_CODE_USE_BEDROCK: "1"
        AWS_REGION: ${{ inputs.aws_region_bedrock }}
        ANTHROPIC_MODEL: ${{ inputs.anthropic_model }}
        AWS_ACCESS_KEY_ID: ${{ inputs.aws_access_key_bedrock }}
        AWS_SECRET_ACCESS_KEY: ${{ inputs.aws_secret_key_bedrock }}
      run: |
        cd ${{ github.action_path }}
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo "â•‘           ğŸ¤– AI ANALYSIS - AWS Bedrock                       â•‘"
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ“‹ Creating requirements document from JIRA and Confluence..."
        echo "   AI Provider: AWS Bedrock"
        echo "   AWS Region: ${AWS_REGION:-not set}"
        echo "   Model: ${ANTHROPIC_MODEL:-default}"
        echo ""
        npm run create-requirement-doc
        echo "âœ… Requirements document created"
        echo ""
        echo "ğŸ” Analyzing unit test quality..."
        npm run analyze-test-quality
        echo "âœ… Test quality analysis completed"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: Extract analysis paths
      id: extract_paths
      shell: bash
      env:
        # ğŸ”’ Security: Use env vars to prevent command injection
        ACTION_PATH: ${{ github.action_path }}
        INPUT_SPACE_KEY: ${{ inputs.confluence_space_key }}
        INPUT_TICKET_ID: ${{ inputs.jira_ticket_id }}
      run: |
        cd "$ACTION_PATH"
        # âœ… SAFE: Sanitize inputs - only allow alphanumeric, hyphen, underscore
        SPACE_KEY=$(echo "$INPUT_SPACE_KEY" | tr -cd 'A-Za-z0-9_-')
        TICKET_ID=$(echo "$INPUT_TICKET_ID" | tr -cd 'A-Za-z0-9_-')

        # Find the latest analysis folder
        ANALYSIS_FOLDER=$(find . -type d -path "*${SPACE_KEY}-Quality-Check-Via-AI/${TICKET_ID}-Via-AI/*-Via-AI" -print | sort -r | head -1)

        if [ -z "$ANALYSIS_FOLDER" ]; then
          echo "âŒ Analysis folder not found"
          exit 1
        fi

        echo "analysis_path=$ANALYSIS_FOLDER" >> $GITHUB_OUTPUT
        echo "requirements_file=$ANALYSIS_FOLDER/Requirements.md" >> $GITHUB_OUTPUT

        # Find result file (could be AnalysisReport.md or AnalysisReport_*.md)
        RESULT_FILE=$(find "$ANALYSIS_FOLDER" -name "AnalysisReport*.md" -type f | head -1)
        echo "result_file=$RESULT_FILE" >> $GITHUB_OUTPUT

        echo "ğŸ“ Analysis folder: $ANALYSIS_FOLDER"
        echo "ğŸ“„ Requirements file: $ANALYSIS_FOLDER/Requirements.md"
        echo "ğŸ“Š Result file: $RESULT_FILE"

    - name: Extract test quality score
      id: extract_score
      shell: bash
      env:
        # ğŸ”’ Security: Use env vars to prevent command injection
        ACTION_PATH: ${{ github.action_path }}
        INPUT_RESULT_FILE: ${{ steps.extract_paths.outputs.result_file }}
      run: |
        cd "$ACTION_PATH"
        # âœ… SAFE: Validate file path doesn't contain malicious characters
        RESULT_FILE=$(echo "$INPUT_RESULT_FILE" | tr -cd 'A-Za-z0-9._/-')

        if [ ! -f "$RESULT_FILE" ]; then
          echo "âŒ Result file not found: $RESULT_FILE"
          exit 1
        fi

        # Method 1: Extract from header "**Total Score:** X.X/10"
        SCORE=$(grep -E "^\*\*Total Score:\*\*" "$RESULT_FILE" | head -1 | grep -oE "[0-9]+\.[0-9]+" | head -1)

        # Method 2: If not found, try from Score Calculation table (TOTAL row)
        if [ -z "$SCORE" ]; then
          echo "   Trying Score Calculation table..."
          SCORE=$(grep -E "^\| \*\*TOTAL\*\*" "$RESULT_FILE" | grep -oE "[0-9]+\.[0-9]+" | head -1)
        fi

        # Method 3: If still not found, try any "Total Score:" pattern
        if [ -z "$SCORE" ]; then
          echo "   Trying generic Total Score pattern..."
          SCORE=$(grep -iE "total score.*[0-9]" "$RESULT_FILE" | grep -oE "[0-9]+\.?[0-9]*" | head -1)
        fi

        # Validate score
        if [ -z "$SCORE" ]; then
          echo "âŒ Could not extract score from result file"
          echo "   Please check the AnalysisReport.md format"
          exit 1
        fi

        # Validate score is within range (0-10)
        VALID=$(awk -v score="$SCORE" 'BEGIN { print (score >= 0 && score <= 10) ? "true" : "false" }')
        if [ "$VALID" != "true" ]; then
          echo "âš ï¸  Score $SCORE is outside valid range (0-10), capping..."
          SCORE=$(awk -v score="$SCORE" 'BEGIN { if (score > 10) print 10; else if (score < 0) print 0; else print score }')
        fi

        echo "score=$SCORE" >> $GITHUB_OUTPUT
        echo "ğŸ“Š Test Quality Score: $SCORE/10"

    - name: Check score threshold
      id: check_score
      shell: bash
      env:
        # ğŸ”’ Security: Use env vars to prevent command injection
        INPUT_SCORE: ${{ steps.extract_score.outputs.score }}
        INPUT_THRESHOLD: ${{ inputs.minimum_score }}
      run: |
        # âœ… SAFE: Validate score is a number (only digits and decimal point)
        SCORE=$(echo "$INPUT_SCORE" | tr -cd '0-9.')
        THRESHOLD=$(echo "$INPUT_THRESHOLD" | tr -cd '0-9.')

        # Compare scores using awk (more portable than bc)
        PASSED=$(awk -v score="$SCORE" -v threshold="$THRESHOLD" 'BEGIN { print (score >= threshold) ? "true" : "false" }')

        echo "passed=$PASSED" >> $GITHUB_OUTPUT

        if [ "$PASSED" = "true" ]; then
          echo "âœ… Test quality score ($SCORE) meets minimum threshold ($THRESHOLD)"
        else
          echo "âš ï¸  Test quality score ($SCORE) is below minimum threshold ($THRESHOLD)"
        fi

    - name: Step 4 - Upload to Confluence
      id: upload_confluence
      if: inputs.upload_to_confluence == 'true'
      shell: bash
      env:
        TEST_QUALITY_SCORE: ${{ steps.extract_score.outputs.score }}
        MINIMUM_SCORE_THRESHOLD: ${{ inputs.minimum_score }}
        SCORE_PASSED: ${{ steps.check_score.outputs.passed }}
      run: |
        cd ${{ github.action_path }}
        echo "ğŸ“¤ Uploading results to Confluence..."
        echo "   Score: ${TEST_QUALITY_SCORE}/10 (Threshold: ${MINIMUM_SCORE_THRESHOLD})"
        OUTPUT=$(npm run upload-requirements 2>&1)
        echo "$OUTPUT"

        # Extract Confluence URL from output
        CONFLUENCE_URL="${{ inputs.confluence_url }}/wiki$(echo "$OUTPUT" | grep -oP 'URL: \K.*' | head -1)"

        if [ -n "$CONFLUENCE_URL" ]; then
          echo "confluence_url=$CONFLUENCE_URL" >> $GITHUB_OUTPUT
          echo "âœ… Results uploaded to Confluence: $CONFLUENCE_URL"
        else
          echo "confluence_url=" >> $GITHUB_OUTPUT
          echo "âœ… Results uploaded to Confluence"
        fi

    - name: Generate summary
      shell: bash
      env:
        # ğŸ”’ Security: Use env vars to prevent command injection
        ACTION_PATH: ${{ github.action_path }}
        INPUT_SCORE: ${{ steps.extract_score.outputs.score }}
        INPUT_THRESHOLD: ${{ inputs.minimum_score }}
        INPUT_PASSED: ${{ steps.check_score.outputs.passed }}
        INPUT_RESULT_FILE: ${{ steps.extract_paths.outputs.result_file }}
        INPUT_CONFLUENCE_URL: ${{ steps.upload_confluence.outputs.confluence_url }}
        INPUT_JIRA_TICKET_ID: ${{ inputs.jira_ticket_id }}
        INPUT_REQUIREMENTS_FILE: ${{ steps.extract_paths.outputs.requirements_file }}
        PR_HTML_URL: ${{ github.event.pull_request.html_url }}
        PR_NUMBER: ${{ github.event.pull_request.number }}
      run: |
        cd "$ACTION_PATH"
        # âœ… SAFE: Sanitize all inputs
        SCORE=$(echo "$INPUT_SCORE" | tr -cd '0-9.')
        THRESHOLD=$(echo "$INPUT_THRESHOLD" | tr -cd '0-9.')
        PASSED=$(echo "$INPUT_PASSED" | tr -cd 'a-z')
        RESULT_FILE=$(echo "$INPUT_RESULT_FILE" | tr -cd 'A-Za-z0-9._/-')
        CONFLUENCE_URL=$(echo "$INPUT_CONFLUENCE_URL" | tr -cd 'A-Za-z0-9._/:?&=-')
        JIRA_TICKET_ID=$(echo "$INPUT_JIRA_TICKET_ID" | tr -cd 'A-Za-z0-9_-')
        REQUIREMENTS_FILE=$(echo "$INPUT_REQUIREMENTS_FILE" | tr -cd 'A-Za-z0-9._/-')
        SAFE_PR_URL=$(echo "$PR_HTML_URL" | tr -cd 'A-Za-z0-9._/:?&=-')
        SAFE_PR_NUMBER=$(echo "$PR_NUMBER" | tr -cd '0-9')

        echo "## ğŸ“Š Unit Test Quality Analysis Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**JIRA Ticket:** $JIRA_TICKET_ID" >> $GITHUB_STEP_SUMMARY
        echo "**Test Quality Score:** ${SCORE}/10" >> $GITHUB_STEP_SUMMARY
        echo "**Minimum Threshold:** ${THRESHOLD}/10" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "$PASSED" = "true" ]; then
          echo "âœ… **Status:** PASSED - Score meets minimum threshold" >> $GITHUB_STEP_SUMMARY
        else
          echo "âš ï¸  **Status:** BELOW THRESHOLD - Additional test coverage needed" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ”— Links" >> $GITHUB_STEP_SUMMARY

        # Add PR link if available (using sanitized env vars)
        if [ -n "$SAFE_PR_URL" ]; then
          echo "- **Pull Request:** [${SAFE_PR_NUMBER}](${SAFE_PR_URL})" >> $GITHUB_STEP_SUMMARY
        fi

        echo "### ğŸ“ Generated Files" >> $GITHUB_STEP_SUMMARY
        echo "- Requirements: \`$REQUIREMENTS_FILE\`" >> $GITHUB_STEP_SUMMARY
        echo "- Analysis Report: \`$RESULT_FILE\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract detailed score breakdown from result file
        echo "### ğŸ“ˆ Detailed Score Breakdown" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract Test Coverage section
        if grep -q "## Test Coverage Analysis" "$RESULT_FILE"; then
          echo "#### 1. Test Coverage" >> $GITHUB_STEP_SUMMARY
          COVERAGE_SCORE=$(grep "## Test Coverage Analysis" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $COVERAGE_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          # Extract key points (lines starting with - under this section until next ##)
          sed -n '/## Test Coverage Analysis/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Test Quality section
        if grep -q "## Test Quality Assessment" "$RESULT_FILE"; then
          echo "#### 2. Test Quality" >> $GITHUB_STEP_SUMMARY
          QUALITY_SCORE=$(grep "## Test Quality Assessment" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $QUALITY_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          sed -n '/## Test Quality Assessment/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Edge Cases section
        if grep -q "## Edge Cases & Error Scenarios" "$RESULT_FILE"; then
          echo "#### 3. Edge Cases & Error Scenarios" >> $GITHUB_STEP_SUMMARY
          EDGE_SCORE=$(grep "## Edge Cases & Error Scenarios" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $EDGE_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          sed -n '/## Edge Cases & Error Scenarios/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Test Assertions section
        if grep -q "## Test Assertions Quality" "$RESULT_FILE"; then
          echo "#### 4. Test Assertions" >> $GITHUB_STEP_SUMMARY
          ASSERTIONS_SCORE=$(grep "## Test Assertions Quality" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $ASSERTIONS_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          sed -n '/## Test Assertions Quality/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Code Organization section
        if grep -q "## Code Organization & Maintainability" "$RESULT_FILE"; then
          echo "#### 5. Code Organization & Maintainability" >> $GITHUB_STEP_SUMMARY
          ORG_SCORE=$(grep "## Code Organization & Maintainability" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $ORG_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          sed -n '/## Code Organization & Maintainability/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Critical Gaps or Main Issues if available
        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if grep -q "### Critical Gaps\|### Main Issues\|### Weaknesses" "$RESULT_FILE"; then
          echo "### âš ï¸  Critical Issues" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          # Try to extract from different possible section names
          sed -n '/### Critical Gaps/,/^## /p' "$RESULT_FILE" | grep "^- \|^[0-9]\." | head -10 >> $GITHUB_STEP_SUMMARY 2>/dev/null || \
          sed -n '/### Main Issues/,/^## /p' "$RESULT_FILE" | grep "^- \|^[0-9]\." | head -10 >> $GITHUB_STEP_SUMMARY 2>/dev/null || \
          sed -n '/### Weaknesses/,/^## /p' "$RESULT_FILE" | grep "^- \|^[0-9]\." | head -10 >> $GITHUB_STEP_SUMMARY 2>/dev/null
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ğŸ“„ **Full detailed report available in job artifacts**" >> $GITHUB_STEP_SUMMARY

    - name: Fail if score below threshold
      if: steps.check_score.outputs.passed == 'false'
      shell: bash
      env:
        # ğŸ”’ Security: Use env vars to prevent command injection
        INPUT_SCORE: ${{ steps.extract_score.outputs.score }}
        INPUT_THRESHOLD: ${{ inputs.minimum_score }}
      run: |
        # âœ… SAFE: Validate score is a number (only digits and decimal point)
        SCORE=$(echo "$INPUT_SCORE" | tr -cd '0-9.')
        THRESHOLD=$(echo "$INPUT_THRESHOLD" | tr -cd '0-9.')

        GAP=$(awk -v threshold="$THRESHOLD" -v score="$SCORE" 'BEGIN { printf "%.1f", threshold - score }')

        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "âŒ TEST QUALITY CHECK FAILED"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ“Š Score: ${SCORE}/10"
        echo "ğŸ¯ Required: ${THRESHOLD}/10"
        echo "ğŸ“‰ Gap: ${GAP} points"
        echo ""
        echo "âš ï¸  This PR cannot be merged until test quality improves."
        echo ""
        echo "ğŸ“‹ Next Steps:"
        echo "  1. Review the analysis report in GitHub Actions artifacts"
        echo "  2. Check Confluence for detailed feedback"
        echo "  3. Add missing test coverage (especially error scenarios)"
        echo "  4. Ensure test quality meets minimum standards"
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""

        # Use GitHub Actions error annotation to make failure visible
        echo "::error::Test quality score (${SCORE}/10) is below minimum threshold (${THRESHOLD}/10). Gap: ${GAP} points. This PR requires additional test coverage before it can be merged."

        exit 1
