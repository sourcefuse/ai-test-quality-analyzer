name: 'Check Unit Test Cases Quality'
description: 'Analyze unit test quality against JIRA requirements and generate comprehensive reports'
author: 'Vishal Gupta'

branding:
  icon: 'check-circle'
  color: 'green'

inputs:
  database_host:
    description: 'PostgreSQL database host'
    required: false
    default: 'localhost'
  database_port:
    description: 'PostgreSQL database port'
    required: false
    default: '5432'
  database_name:
    description: 'PostgreSQL database name'
    required: false
    default: 'postgres-pgvector'
  database_user:
    description: 'PostgreSQL database user'
    required: false
    default: 'postgres'
  database_password:
    description: 'PostgreSQL database password'
    required: false
    default: 'admin'
  jira_url:
    description: 'JIRA instance URL'
    required: true
  jira_email:
    description: 'JIRA user email'
    required: true
  jira_api_token:
    description: 'JIRA API token'
    required: true
  jira_ticket_id:
    description: 'JIRA ticket ID to analyze (e.g., BB-15690)'
    required: true
  jira_project_key:
    description: 'JIRA project key'
    required: true
  confluence_url:
    description: 'Confluence instance URL'
    required: true
  confluence_email:
    description: 'Confluence user email'
    required: true
  confluence_api_token:
    description: 'Confluence API token'
    required: true
  confluence_space_key:
    description: 'Confluence space key'
    required: true
  confluence_silent_mode:
    description: 'Suppress logging during Confluence page downloads (true/false)'
    required: false
    default: 'true'
  current_analysis_path:
    description: 'Specific analysis folder to use (optional, auto-generated if not provided)'
    required: false
    default: ''
  repository_url:
    description: 'Repository URL to clone for test analysis'
    required: true
  repository_branch:
    description: 'Branch name to clone'
    required: false
    default: 'main'
  save_to_file:
    description: 'Save analysis results to files (true/false)'
    required: false
    default: 'true'
  upload_to_confluence:
    description: 'Upload results to Confluence (true/false)'
    required: false
    default: 'true'
  minimum_score:
    description: 'Minimum acceptable test quality score (0-10)'
    required: false
    default: '6.0'
  aws_region_bedrock:
    description: 'AWS region for Bedrock service'
    required: false
    default: 'us-east-2'
  aws_access_key_bedrock:
    description: 'AWS access key for Bedrock authentication'
    required: true
  aws_secret_key_bedrock:
    description: 'AWS secret key for Bedrock authentication'
    required: true
  aws_bedrock_model:
    description: 'AWS Bedrock model identifier'
    required: false
    default: 'us.anthropic.claude-3-5-sonnet-20241022-v2:0'
  anthropic_model:
    description: 'Anthropic model name for Claude'
    required: false
    default: 'sonnet[1m]'
  claude_code_use_bedrock:
    description: 'Enable AWS Bedrock for Claude (1=enabled, 0=disabled)'
    required: false
    default: '1'
  base_folder_suffix:
    description: 'Base folder suffix for organizing analysis results'
    required: false
    default: 'Quality-Check-Via-AI'
  confluence_root_page_suffix:
    description: 'Confluence root page suffix for organizing reports'
    required: false
    default: 'Quality-Check-Via-AI'
  confluence_upload_url:
    description: 'Confluence instance URL for uploading results (if different from fetch URL)'
    required: false
    default: ''
  confluence_upload_email:
    description: 'Confluence user email for uploading results (if different from fetch email)'
    required: false
    default: ''
  confluence_upload_api_token:
    description: 'Confluence API token for uploading results (if different from fetch token)'
    required: false
    default: ''
  confluence_upload_space_key:
    description: 'Confluence space key for uploading results (if different from fetch space)'
    required: false
    default: ''
  docker_username:
    description: 'Docker registry username for authentication (optional - required for Presidio PII detection)'
    required: false
    default: ''
  docker_password:
    description: 'Docker registry password/token for authentication (optional - required for Presidio PII detection)'
    required: false
    default: ''
  openai_api_key:
    description: 'OpenAI API key for embeddings (required if USE_POSTGRES_VECTOR_DB is enabled)'
    required: false
    default: ''
  use_postgres_vector_db:
    description: 'Enable PostgreSQL Vector Database with pgvector for RAG-based Confluence fetching (true/false)'
    required: false
    default: 'true'

outputs:
  test_quality_score:
    description: 'Test quality score (0-10)'
    value: ${{ steps.extract_score.outputs.score }}
  analysis_path:
    description: 'Path to the analysis results folder'
    value: ${{ steps.extract_paths.outputs.analysis_path }}
  requirements_file:
    description: 'Path to the requirements file'
    value: ${{ steps.extract_paths.outputs.requirements_file }}
  result_file:
    description: 'Path to the result file'
    value: ${{ steps.extract_paths.outputs.result_file }}
  score_passed:
    description: 'Whether the score meets the minimum threshold (true/false)'
    value: ${{ steps.check_score.outputs.passed }}
  confluence_url:
    description: 'Confluence URL to the uploaded analysis report'
    value: ${{ steps.upload_confluence.outputs.confluence_url }}

runs:
  using: 'composite'
  steps:
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'

    - name: Install dependencies
      shell: bash
      run: |
        cd ${{ github.action_path }}
        npm install
        npm install -g @anthropic-ai/claude-code

    - name: Setup PostgreSQL Database
      if: inputs.database_host == 'localhost' && inputs.docker_username != '' && inputs.docker_password != ''
      shell: bash
      run: |
        echo "ğŸ˜ Setting up PostgreSQL database..."

        # Pull PostgreSQL image with pgvector pre-installed
        echo "ğŸ“¥ Pulling PostgreSQL + pgvector image..."
        docker pull ankane/pgvector:latest -q 2>&1 | grep -v "Pulling\|Waiting\|Downloading\|Extracting\|Verifying\|Pull complete" || true

        # Start PostgreSQL container
        echo "ğŸš€ Starting PostgreSQL container..."
        docker run -d \
          --name postgres-pgvector \
          -e POSTGRES_USER="${{ inputs.database_user }}" \
          -e POSTGRES_PASSWORD="${{ inputs.database_password }}" \
          -e POSTGRES_DB="${{ inputs.database_name }}" \
          -p ${{ inputs.database_port }}:5432 \
          ankane/pgvector:latest

        # Wait for PostgreSQL to be ready
        echo "â³ Waiting for PostgreSQL to be ready..."
        sleep 10

        # Verify PostgreSQL is running
        echo "âœ… PostgreSQL setup complete"
        docker ps | grep postgres-pgvector

    - name: Docker login and start Presidio services
      if: inputs.docker_username != '' && inputs.docker_password != ''
      shell: bash
      run: |
        echo "ğŸ³ Logging in to Docker registry..."
        echo "${{ inputs.docker_password }}" | docker login -u "${{ inputs.docker_username }}" --password-stdin docker.io

        echo "ğŸ“¥ Pulling Presidio Docker images..."
        docker pull mcr.microsoft.com/presidio-analyzer:latest -q 2>&1 | grep -v "Pulling\|Waiting\|Downloading\|Extracting\|Verifying\|Pull complete" || true
        docker pull mcr.microsoft.com/presidio-anonymizer:latest -q 2>&1 | grep -v "Pulling\|Waiting\|Downloading\|Extracting\|Verifying\|Pull complete" || true

        echo "ğŸš€ Starting Presidio services..."
        docker run -d -p 5001:3000 --name presidio-anonymizer mcr.microsoft.com/presidio-anonymizer:latest
        docker run -d -p 5002:3000 --name presidio-analyzer mcr.microsoft.com/presidio-analyzer:latest

        echo "â³ Waiting for services to be ready..."
        sleep 10

        echo "ğŸ” Testing Presidio services availability..."

        # Wait up to 60 seconds for Presidio Analyzer to be ready
        MAX_WAIT=60
        ELAPSED=0
        ANALYZER_READY=false

        while [ $ELAPSED -lt $MAX_WAIT ]; do
          if curl -s -X POST http://localhost:5002/analyze \
            -H "Content-Type: application/json" \
            -d '{"text":"test","language":"en"}' \
            --max-time 5 > /dev/null 2>&1; then
            ANALYZER_READY=true
            echo "âœ… Presidio Analyzer is ready (took ${ELAPSED}s)"
            break
          fi
          echo "   â³ Waiting for Presidio Analyzer... (${ELAPSED}s/${MAX_WAIT}s)"
          sleep 5
          ELAPSED=$((ELAPSED + 5))
        done

        if [ "$ANALYZER_READY" = false ]; then
          echo "âš ï¸  Presidio Analyzer did not respond within ${MAX_WAIT}s - PII detection will use regex fallback"
        fi

        # Wait up to 60 seconds for Presidio Anonymizer to be ready
        ELAPSED=0
        ANONYMIZER_READY=false

        while [ $ELAPSED -lt $MAX_WAIT ]; do
          if curl -s -X POST http://localhost:5001/anonymize \
            -H "Content-Type: application/json" \
            -d '{"text":"test","anonymizers":{}}' \
            --max-time 5 > /dev/null 2>&1; then
            ANONYMIZER_READY=true
            echo "âœ… Presidio Anonymizer is ready (took ${ELAPSED}s)"
            break
          fi
          echo "   â³ Waiting for Presidio Anonymizer... (${ELAPSED}s/${MAX_WAIT}s)"
          sleep 5
          ELAPSED=$((ELAPSED + 5))
        done

        if [ "$ANONYMIZER_READY" = false ]; then
          echo "âš ï¸  Presidio Anonymizer did not respond within ${MAX_WAIT}s - PII detection will use regex fallback"
        fi

        if [ "$ANALYZER_READY" = true ] && [ "$ANONYMIZER_READY" = true ]; then
          echo "âœ… All Presidio services are ready and responding"
        else
          echo "âš ï¸  Some Presidio services are not ready - application will use regex-based PII detection"
        fi

        echo ""
        echo "ğŸ“Š Presidio containers status:"
        docker ps | grep presidio

    - name: Skip Presidio setup notification
      if: inputs.docker_username == '' || inputs.docker_password == ''
      shell: bash
      run: |
        echo "âš ï¸  Skipping Docker login and Presidio setup"
        echo "   Docker credentials not provided - PII detection will be disabled"
        echo "   To enable Presidio services, provide docker_username and docker_password inputs"

    - name: Clone repository for analysis
      shell: bash
      run: |
        cd ${{ github.action_path }}
        echo "ğŸ“¥ Cloning repository for test analysis..."

        # Use head_ref for PRs (gives branch name like BB-2244), ref_name for other events
        BRANCH="${{ github.head_ref || github.ref_name }}"
        echo "Branch to clone: $BRANCH"

        mkdir -p repo
        cd repo
        git clone --depth 1 --branch "$BRANCH" --single-branch https://x-access-token:${{ github.token }}@github.com/${{ github.repository }}.git .
        echo "âœ… Repository cloned successfully (branch: $BRANCH)"

    - name: Create .env file
      shell: bash
      run: |
        cd ${{ github.action_path }}
        cat > .env << EOF
        # Application Configuration
        NODE_ENV=production

        # JIRA Configuration
        JIRA_URL=${{ inputs.jira_url }}
        JIRA_EMAIL=${{ inputs.jira_email }}
        JIRA_API_TOKEN=${{ inputs.jira_api_token }}
        JIRA_TICKET_ID=${{ inputs.jira_ticket_id }}
        JIRA_PROJECT_KEY=${{ inputs.jira_project_key }}
        JIRA_FETCH_FIELDS=summary,description,customfield_10000
        JIRA_MAX_RESULT=100
        JIRA_FILE_NAME=Jira.md

        # Confluence Configuration (for fetching data)
        CONFLUENCE_URL=${{ inputs.confluence_url }}
        CONFLUENCE_EMAIL=${{ inputs.confluence_email }}
        CONFLUENCE_API_TOKEN=${{ inputs.confluence_api_token }}
        CONFLUENCE_SPACE_KEY=${{ inputs.confluence_space_key }}
        CONFLUENCE_SILENT_MODE=${{ inputs.confluence_silent_mode }}
        CONFLUENCE_MAX_PAGES=
        CONFLUENCE_PAGE_LIMIT=50
        CONFLUENCE_FILE_NAME=Confluence.md
        CONFLUENCE_RAG_FILE_NAME=Confluence-Rag.md

        # Confluence Upload Configuration (for uploading results)
        CONFLUENCE_UPLOAD_URL=${{ inputs.confluence_upload_url }}
        CONFLUENCE_UPLOAD_EMAIL=${{ inputs.confluence_upload_email }}
        CONFLUENCE_UPLOAD_API_TOKEN=${{ inputs.confluence_upload_api_token }}
        CONFLUENCE_UPLOAD_SPACE_KEY=${{ inputs.confluence_upload_space_key }}

        # Docker Configuration
        DOCKER_USERNAME=${{ inputs.docker_username }}
        DOCKER_PASSWORD=${{ inputs.docker_password }}

        # Presidio Configuration (PII Detection)
        PRESIDIO_ANALYZE_URL=http://localhost:5002/analyze
        PRESIDIO_ANONYMIZE_URL=http://localhost:5001/anonymize

        # Data Sanitization Configuration
        SANITIZE_PG_DATA=false

        # AWS Bedrock Configuration
        AWS_REGION_BEDROCK=${{ inputs.aws_region_bedrock }}
        AWS_ACCESS_KEY_BEDROCK=${{ inputs.aws_access_key_bedrock }}
        AWS_SECRET_KEY_BEDROCK=${{ inputs.aws_secret_key_bedrock }}
        AWS_BEDROCK_MODEL=${{ inputs.aws_bedrock_model }}
        ANTHROPIC_MODEL=${{ inputs.anthropic_model }}
        CLAUDE_CODE_USE_BEDROCK=${{ inputs.claude_code_use_bedrock }}
        BASE_FOLDER_SUFFIX=${{ inputs.base_folder_suffix }}
        CONFLUENCE_ROOT_PAGE_SUFFIX=${{ inputs.confluence_root_page_suffix }}

        # Analysis Configuration
        CURRENT_ANALYSIS_PATH=$(echo $(date +'%Y%m%d-%H-%M-%S')-Via-AI)
        SAVE_TO_FILE=${{ inputs.save_to_file }}

        # File Names Configuration
        REQUIREMENTS_FILE_NAME=Requirements.md
        ANALYSIS_REPORT_FILE_NAME=AnalysisReport.md

        # RAG System Configuration (PostgreSQL + pgvector)
        USE_POSTGRES_VECTOR_DB=${{ inputs.use_postgres_vector_db }}
        DATABASE_HOST=${{ inputs.database_host }}
        DATABASE_PORT=${{ inputs.database_port }}
        DATABASE_NAME=${{ inputs.database_name }}
        DATABASE_USER=${{ inputs.database_user }}
        DATABASE_PASSWORD=${{ inputs.database_password }}

        # OpenAI Embeddings Configuration
        EMBEDDING_PROVIDER=openai
        OPENAI_API_KEY=${{ inputs.openai_api_key }}

        # Performance Configuration
        EMBEDDING_CONCURRENCY=50
        INDEXER_BATCH_SIZE=50
        CHUNK_SIZE=1000
        CHUNK_OVERLAP=200

        # Smart Filter Configuration
        USE_SMART_FILTER=true
        SMART_FILTER_MAX_PAGES=30
        SMART_FILTER_MIN_SCORE=0.3
        SMART_FILTER_USE_KEYWORDS=true
        SMART_FILTER_USE_TITLE=true
        SMART_FILTER_USE_LABELS=true
        SMART_FILTER_USE_COMPONENTS=true
        SMART_FILTER_DEBUG=false
        CHECK_PG_BEFORE_CONFLUENCE_FETCH=true

        EOF
        echo "âœ… Environment configuration created"

    - name: Step 1 - Fetch JIRA and Confluence data
      shell: bash
      run: |
        cd ${{ github.action_path }}
        echo "ğŸ”„ Fetching JIRA ticket and Confluence pages..."
        npm run start
        echo "âœ… Data fetched successfully"

    - name: Step 2 - Create Requirements document
      if: inputs.claude_code_use_bedrock == '1'
      shell: bash
      env:
        CLAUDE_CODE_USE_BEDROCK:  "1"
        AWS_REGION:               ${{ inputs.aws_region_bedrock }}
        ANTHROPIC_MODEL:          ${{ inputs.anthropic_model }}
        AWS_ACCESS_KEY_ID:        ${{ inputs.aws_access_key_bedrock }}
        AWS_SECRET_ACCESS_KEY:    ${{ inputs.aws_secret_key_bedrock }}
      run: |
        cd ${{ github.action_path }}
        echo "ğŸ“‹ Creating requirements document from JIRA and Confluence..."
        npm run create-requirement-doc
        echo "âœ… Requirements document created"

        echo "ğŸ” Analyzing unit test quality..."
        npm run analyze-test-quality
        echo "âœ… Test quality analysis completed"

    - name: Extract analysis paths
      id: extract_paths
      shell: bash
      run: |
        cd ${{ github.action_path }}
        SPACE_KEY="${{ inputs.confluence_space_key }}"
        TICKET_ID="${{ inputs.jira_ticket_id }}"

        # Find the latest analysis folder
        ANALYSIS_FOLDER=$(find . -type d -path "*${SPACE_KEY}-Quality-Check-Via-AI/${TICKET_ID}-Via-AI/*-Via-AI" -print | sort -r | head -1)

        if [ -z "$ANALYSIS_FOLDER" ]; then
          echo "âŒ Analysis folder not found"
          exit 1
        fi

        echo "analysis_path=$ANALYSIS_FOLDER" >> $GITHUB_OUTPUT
        echo "requirements_file=$ANALYSIS_FOLDER/Requirements.md" >> $GITHUB_OUTPUT

        # Find result file (could be AnalysisReport.md or AnalysisReport_*.md)
        RESULT_FILE=$(find "$ANALYSIS_FOLDER" -name "AnalysisReport*.md" -type f | head -1)
        echo "result_file=$RESULT_FILE" >> $GITHUB_OUTPUT

        echo "ğŸ“ Analysis folder: $ANALYSIS_FOLDER"
        echo "ğŸ“„ Requirements file: $ANALYSIS_FOLDER/Requirements.md"
        echo "ğŸ“Š Result file: $RESULT_FILE"

    - name: Extract test quality score
      id: extract_score
      shell: bash
      run: |
        cd ${{ github.action_path }}
        RESULT_FILE="${{ steps.extract_paths.outputs.result_file }}"

        if [ ! -f "$RESULT_FILE" ]; then
          echo "âŒ Result file not found: $RESULT_FILE"
          exit 1
        fi

        # Extract the numeric score (e.g., "3.5" from "**Total Score:** 3.5/10")
        SCORE=$(grep -E "^\*\*Total Score:\*\*|^Total Score:" "$RESULT_FILE" | grep -oE "[0-9]+(\.[0-9]+)?" | head -1)

        if [ -z "$SCORE" ]; then
          echo "âŒ Could not extract score from result file"
          exit 1
        fi

        echo "score=$SCORE" >> $GITHUB_OUTPUT
        echo "ğŸ“Š Test Quality Score: $SCORE/10"

    - name: Check score threshold
      id: check_score
      shell: bash
      run: |
        SCORE="${{ steps.extract_score.outputs.score }}"
        THRESHOLD="${{ inputs.minimum_score }}"

        # Compare scores using awk (more portable than bc)
        PASSED=$(awk -v score="$SCORE" -v threshold="$THRESHOLD" 'BEGIN { print (score >= threshold) ? "true" : "false" }')

        echo "passed=$PASSED" >> $GITHUB_OUTPUT

        if [ "$PASSED" = "true" ]; then
          echo "âœ… Test quality score ($SCORE) meets minimum threshold ($THRESHOLD)"
        else
          echo "âš ï¸  Test quality score ($SCORE) is below minimum threshold ($THRESHOLD)"
        fi

    - name: Step 4 - Upload to Confluence
      id: upload_confluence
      if: inputs.upload_to_confluence == 'true'
      shell: bash
      env:
        TEST_QUALITY_SCORE: ${{ steps.extract_score.outputs.score }}
        MINIMUM_SCORE_THRESHOLD: ${{ inputs.minimum_score }}
        SCORE_PASSED: ${{ steps.check_score.outputs.passed }}
      run: |
        cd ${{ github.action_path }}
        echo "ğŸ“¤ Uploading results to Confluence..."
        echo "   Score: ${TEST_QUALITY_SCORE}/10 (Threshold: ${MINIMUM_SCORE_THRESHOLD})"
        OUTPUT=$(npm run upload-requirements 2>&1)
        echo "$OUTPUT"

        # Extract Confluence URL from output
        CONFLUENCE_URL="${{ inputs.confluence_url }}/wiki$(echo "$OUTPUT" | grep -oP 'URL: \K.*' | head -1)"

        if [ -n "$CONFLUENCE_URL" ]; then
          echo "confluence_url=$CONFLUENCE_URL" >> $GITHUB_OUTPUT
          echo "âœ… Results uploaded to Confluence: $CONFLUENCE_URL"
        else
          echo "confluence_url=" >> $GITHUB_OUTPUT
          echo "âœ… Results uploaded to Confluence"
        fi

    - name: Generate summary
      shell: bash
      run: |
        cd ${{ github.action_path }}
        SCORE="${{ steps.extract_score.outputs.score }}"
        THRESHOLD="${{ inputs.minimum_score }}"
        PASSED="${{ steps.check_score.outputs.passed }}"
        RESULT_FILE="${{ steps.extract_paths.outputs.result_file }}"
        CONFLUENCE_URL="${{ steps.upload_confluence.outputs.confluence_url }}"

        echo "## ğŸ“Š Unit Test Quality Analysis Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**JIRA Ticket:** ${{ inputs.jira_ticket_id }}" >> $GITHUB_STEP_SUMMARY
        echo "**Test Quality Score:** ${SCORE}/10" >> $GITHUB_STEP_SUMMARY
        echo "**Minimum Threshold:** ${THRESHOLD}/10" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "$PASSED" = "true" ]; then
          echo "âœ… **Status:** PASSED - Score meets minimum threshold" >> $GITHUB_STEP_SUMMARY
        else
          echo "âš ï¸  **Status:** BELOW THRESHOLD - Additional test coverage needed" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ”— Links" >> $GITHUB_STEP_SUMMARY

        # Add PR link if available
        if [ -n "${{ github.event.pull_request.html_url }}" ]; then
          echo "- **Pull Request:** [${{ github.event.pull_request.number }}](${{ github.event.pull_request.html_url }})" >> $GITHUB_STEP_SUMMARY
        fi

        # Add Confluence link if upload was successful
        if [ -n "$CONFLUENCE_URL" ]; then
          echo "- **Confluence Report:** [View Analysis Report](${CONFLUENCE_URL})" >> $GITHUB_STEP_SUMMARY
        fi

        # Add GitHub Actions artifacts link
        echo "- **GitHub Artifacts:** [Download Reports](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### ğŸ“ Generated Files" >> $GITHUB_STEP_SUMMARY
        echo "- Requirements: \`${{ steps.extract_paths.outputs.requirements_file }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Analysis Report: \`$RESULT_FILE\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract detailed score breakdown from result file
        echo "### ğŸ“ˆ Detailed Score Breakdown" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract Test Coverage section
        if grep -q "## Test Coverage Analysis" "$RESULT_FILE"; then
          echo "#### 1. Test Coverage" >> $GITHUB_STEP_SUMMARY
          COVERAGE_SCORE=$(grep "## Test Coverage Analysis" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $COVERAGE_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          # Extract key points (lines starting with - under this section until next ##)
          sed -n '/## Test Coverage Analysis/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Test Quality section
        if grep -q "## Test Quality Assessment" "$RESULT_FILE"; then
          echo "#### 2. Test Quality" >> $GITHUB_STEP_SUMMARY
          QUALITY_SCORE=$(grep "## Test Quality Assessment" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $QUALITY_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          sed -n '/## Test Quality Assessment/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Edge Cases section
        if grep -q "## Edge Cases & Error Scenarios" "$RESULT_FILE"; then
          echo "#### 3. Edge Cases & Error Scenarios" >> $GITHUB_STEP_SUMMARY
          EDGE_SCORE=$(grep "## Edge Cases & Error Scenarios" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $EDGE_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          sed -n '/## Edge Cases & Error Scenarios/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Test Assertions section
        if grep -q "## Test Assertions Quality" "$RESULT_FILE"; then
          echo "#### 4. Test Assertions" >> $GITHUB_STEP_SUMMARY
          ASSERTIONS_SCORE=$(grep "## Test Assertions Quality" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $ASSERTIONS_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          sed -n '/## Test Assertions Quality/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Code Organization section
        if grep -q "## Code Organization & Maintainability" "$RESULT_FILE"; then
          echo "#### 5. Code Organization & Maintainability" >> $GITHUB_STEP_SUMMARY
          ORG_SCORE=$(grep "## Code Organization & Maintainability" "$RESULT_FILE" | grep -oE "\([0-9.]+/[0-9.]+ points\)" | head -1)
          echo "**Score:** $ORG_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          sed -n '/## Code Organization & Maintainability/,/^## /p' "$RESULT_FILE" | grep "^- " | head -5 >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract Critical Gaps or Main Issues if available
        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if grep -q "### Critical Gaps\|### Main Issues\|### Weaknesses" "$RESULT_FILE"; then
          echo "### âš ï¸  Critical Issues" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          # Try to extract from different possible section names
          sed -n '/### Critical Gaps/,/^## /p' "$RESULT_FILE" | grep "^- \|^[0-9]\." | head -10 >> $GITHUB_STEP_SUMMARY 2>/dev/null || \
          sed -n '/### Main Issues/,/^## /p' "$RESULT_FILE" | grep "^- \|^[0-9]\." | head -10 >> $GITHUB_STEP_SUMMARY 2>/dev/null || \
          sed -n '/### Weaknesses/,/^## /p' "$RESULT_FILE" | grep "^- \|^[0-9]\." | head -10 >> $GITHUB_STEP_SUMMARY 2>/dev/null
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ğŸ“„ **Full detailed report available in job artifacts**" >> $GITHUB_STEP_SUMMARY

    - name: Fail if score below threshold
      if: steps.check_score.outputs.passed == 'false'
      shell: bash
      run: |
        SCORE="${{ steps.extract_score.outputs.score }}"
        THRESHOLD="${{ inputs.minimum_score }}"

        GAP=$(awk -v threshold="$THRESHOLD" -v score="$SCORE" 'BEGIN { printf "%.1f", threshold - score }')

        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "âŒ TEST QUALITY CHECK FAILED"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ“Š Score: ${SCORE}/10"
        echo "ğŸ¯ Required: ${THRESHOLD}/10"
        echo "ğŸ“‰ Gap: ${GAP} points"
        echo ""
        echo "âš ï¸  This PR cannot be merged until test quality improves."
        echo ""
        echo "ğŸ“‹ Next Steps:"
        echo "  1. Review the analysis report in GitHub Actions artifacts"
        echo "  2. Check Confluence for detailed feedback"
        echo "  3. Add missing test coverage (especially error scenarios)"
        echo "  4. Ensure test quality meets minimum standards"
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""

        # Use GitHub Actions error annotation to make failure visible
        echo "::error::Test quality score (${SCORE}/10) is below minimum threshold (${THRESHOLD}/10). Gap: ${GAP} points. This PR requires additional test coverage before it can be merged."

        exit 1
