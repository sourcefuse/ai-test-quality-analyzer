name: 'Check Unit Test Cases Quality'
description: 'Analyze unit test quality against JIRA requirements and generate comprehensive reports'
author: 'Vishal Gupta'

branding:
  icon: 'check-circle'
  color: 'green'

inputs:
  jira_url:
    description: 'JIRA instance URL'
    required: true
  jira_email:
    description: 'JIRA user email'
    required: true
  jira_api_token:
    description: 'JIRA API token'
    required: true
  jira_ticket_id:
    description: 'JIRA ticket ID to analyze (e.g., BB-15690)'
    required: true
  jira_project_key:
    description: 'JIRA project key'
    required: true
  confluence_url:
    description: 'Confluence instance URL'
    required: true
  confluence_email:
    description: 'Confluence user email'
    required: true
  confluence_api_token:
    description: 'Confluence API token'
    required: true
  confluence_space_key:
    description: 'Confluence space key'
    required: true
  confluence_silent_mode:
    description: 'Suppress logging during Confluence page downloads (true/false)'
    required: false
    default: 'true'
  current_analysis_path:
    description: 'Specific analysis folder to use (optional, auto-generated if not provided)'
    required: false
    default: ''
  repository_url:
    description: 'Repository URL to clone for test analysis'
    required: true
  repository_branch:
    description: 'Branch name to clone'
    required: false
    default: 'main'
  save_to_file:
    description: 'Save analysis results to files (true/false)'
    required: false
    default: 'true'
  upload_to_confluence:
    description: 'Upload results to Confluence (true/false)'
    required: false
    default: 'true'
  minimum_score:
    description: 'Minimum acceptable test quality score (0-10)'
    required: false
    default: '6.0'
  aws_region_bedrock:
    description: 'AWS region for Bedrock service'
    required: false
    default: 'us-east-2'
  aws_access_key_bedrock:
    description: 'AWS access key for Bedrock authentication'
    required: true
  aws_secret_key_bedrock:
    description: 'AWS secret key for Bedrock authentication'
    required: true
  aws_bedrock_model:
    description: 'AWS Bedrock model identifier'
    required: false
    default: 'us.anthropic.claude-3-5-sonnet-20241022-v2:0'
  anthropic_model:
    description: 'Anthropic model name for Claude'
    required: false
    default: 'sonnet[1m]'
  claude_code_use_bedrock:
    description: 'Enable AWS Bedrock for Claude (1=enabled, 0=disabled)'
    required: false
    default: '1'

outputs:
  test_quality_score:
    description: 'Test quality score (0-10)'
    value: ${{ steps.extract_score.outputs.score }}
  analysis_path:
    description: 'Path to the analysis results folder'
    value: ${{ steps.extract_paths.outputs.analysis_path }}
  requirements_file:
    description: 'Path to the requirements file'
    value: ${{ steps.extract_paths.outputs.requirements_file }}
  result_file:
    description: 'Path to the result file'
    value: ${{ steps.extract_paths.outputs.result_file }}
  score_passed:
    description: 'Whether the score meets the minimum threshold (true/false)'
    value: ${{ steps.check_score.outputs.passed }}
  confluence_url:
    description: 'Confluence URL to the uploaded analysis report'
    value: ${{ steps.upload_confluence.outputs.confluence_url }}

runs:
  using: 'composite'
  steps:
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'

    - name: Install dependencies
      shell: bash
      run: |
        cd ${{ github.action_path }}
        npm install
        npm install -g @anthropic-ai/claude-code

    - name: Clone repository for analysis
      shell: bash
      run: |
        cd ${{ github.action_path }}
        echo "üì• Cloning repository for test analysis..."

        # Use head_ref for PRs (gives branch name like BB-2244), ref_name for other events
        BRANCH="${{ github.head_ref || github.ref_name }}"
        echo "Branch to clone: $BRANCH"

        mkdir -p repo
        cd repo
        git clone --depth 1 --branch "$BRANCH" --single-branch https://x-access-token:${{ github.token }}@github.com/${{ github.repository }}.git .
        echo "‚úÖ Repository cloned successfully (branch: $BRANCH)"

    - name: Create .env file
      shell: bash
      run: |
        cd ${{ github.action_path }}
        cat > .env << EOF
        # JIRA Configuration
        JIRA_URL=${{ inputs.jira_url }}
        JIRA_EMAIL=${{ inputs.jira_email }}
        JIRA_API_TOKEN=${{ inputs.jira_api_token }}
        JIRA_TICKET_ID=${{ inputs.jira_ticket_id }}
        JIRA_PROJECT_KEY=${{ inputs.jira_project_key }}

        # Confluence Configuration
        CONFLUENCE_URL=${{ inputs.confluence_url }}
        CONFLUENCE_EMAIL=${{ inputs.confluence_email }}
        CONFLUENCE_API_TOKEN=${{ inputs.confluence_api_token }}
        CONFLUENCE_SPACE_KEY=${{ inputs.confluence_space_key }}
        CONFLUENCE_SILENT_MODE=${{ inputs.confluence_silent_mode }}

        # AWS Bedrock Configuration
        AWS_REGION_BEDROCK=${{ inputs.aws_region_bedrock }}
        AWS_ACCESS_KEY_BEDROCK=${{ inputs.aws_access_key_bedrock }}
        AWS_SECRET_KEY_BEDROCK=${{ inputs.aws_secret_key_bedrock }}
        AWS_BEDROCK_MODEL=${{ inputs.aws_bedrock_model }}
        ANTHROPIC_MODEL=${{ inputs.anthropic_model }}
        CLAUDE_CODE_USE_BEDROCK=${{ inputs.claude_code_use_bedrock }}

        # Analysis Configuration
        CURRENT_ANALYSIS_PATH=$(echo $(date +'%Y%m%d-%H-%M-%S')-Via-AI)
        SAVE_TO_FILE=${{ inputs.save_to_file }}
        EOF
        echo "‚úÖ Environment configuration created"

    - name: Step 1 - Fetch JIRA and Confluence data
      shell: bash
      run: |
        cd ${{ github.action_path }}
        echo "üîÑ Fetching JIRA ticket and Confluence pages..."
        npm run start
        echo "‚úÖ Data fetched successfully"

    - name: Step 2 - Create Requirements document
      if: inputs.claude_code_use_bedrock == '1'
      shell: bash
      env:
        CLAUDE_CODE_USE_BEDROCK:  "1"
        AWS_REGION:               ${{ inputs.aws_region_bedrock }}
        ANTHROPIC_MODEL:          ${{ inputs.anthropic_model }}
        AWS_ACCESS_KEY_ID:        ${{ inputs.aws_access_key_bedrock }}
        AWS_SECRET_ACCESS_KEY:    ${{ inputs.aws_secret_key_bedrock }}
      run: |
        cd ${{ github.action_path }}
        echo "üìã Creating requirements document from JIRA and Confluence..."
        npm run create-requirement-doc
        echo "‚úÖ Requirements document created"

        echo "üîç Analyzing unit test quality..."
        npm run analyze-test-quality
        echo "‚úÖ Test quality analysis completed"

    - name: Extract analysis paths
      id: extract_paths
      shell: bash
      run: |
        cd ${{ github.action_path }}
        SPACE_KEY="${{ inputs.confluence_space_key }}"
        TICKET_ID="${{ inputs.jira_ticket_id }}"

        # Find the latest analysis folder
        ANALYSIS_FOLDER=$(find . -type d -path "*${SPACE_KEY}-Quality-Check-Via-AI/${TICKET_ID}-Via-AI/*-Via-AI" -print | sort -r | head -1)

        if [ -z "$ANALYSIS_FOLDER" ]; then
          echo "‚ùå Analysis folder not found"
          exit 1
        fi

        echo "analysis_path=$ANALYSIS_FOLDER" >> $GITHUB_OUTPUT
        echo "requirements_file=$ANALYSIS_FOLDER/Requirements.md" >> $GITHUB_OUTPUT

        # Find result file (could be AnalysisReport.md or AnalysisReport_*.md)
        RESULT_FILE=$(find "$ANALYSIS_FOLDER" -name "AnalysisReport*.md" -type f | head -1)
        echo "result_file=$RESULT_FILE" >> $GITHUB_OUTPUT

        echo "üìÅ Analysis folder: $ANALYSIS_FOLDER"
        echo "üìÑ Requirements file: $ANALYSIS_FOLDER/Requirements.md"
        echo "üìä Result file: $RESULT_FILE"

    - name: Extract test quality score
      id: extract_score
      shell: bash
      run: |
        cd ${{ github.action_path }}
        RESULT_FILE="${{ steps.extract_paths.outputs.result_file }}"

        if [ ! -f "$RESULT_FILE" ]; then
          echo "‚ùå Result file not found: $RESULT_FILE"
          exit 1
        fi

        # Extract the numeric score (e.g., "3.5" from "**Total Score:** 3.5/10")
        SCORE=$(grep -E "^\*\*Total Score:\*\*|^Total Score:" "$RESULT_FILE" | grep -oE "[0-9]+(\.[0-9]+)?" | head -1)

        if [ -z "$SCORE" ]; then
          echo "‚ùå Could not extract score from result file"
          exit 1
        fi

        echo "score=$SCORE" >> $GITHUB_OUTPUT
        echo "üìä Test Quality Score: $SCORE/10"

    - name: Check score threshold
      id: check_score
      shell: bash
      run: |
        SCORE="${{ steps.extract_score.outputs.score }}"
        THRESHOLD="${{ inputs.minimum_score }}"

        # Compare scores using awk (more portable than bc)
        PASSED=$(awk -v score="$SCORE" -v threshold="$THRESHOLD" 'BEGIN { print (score >= threshold) ? "true" : "false" }')

        echo "passed=$PASSED" >> $GITHUB_OUTPUT

        if [ "$PASSED" = "true" ]; then
          echo "‚úÖ Test quality score ($SCORE) meets minimum threshold ($THRESHOLD)"
        else
          echo "‚ö†Ô∏è  Test quality score ($SCORE) is below minimum threshold ($THRESHOLD)"
        fi

    - name: Step 4 - Upload to Confluence
      id: upload_confluence
      if: inputs.upload_to_confluence == 'true'
      shell: bash
      run: |
        cd ${{ github.action_path }}
        echo "üì§ Uploading results to Confluence..."
        OUTPUT=$(npm run upload-requirements 2>&1)
        echo "$OUTPUT"

        # Extract Confluence URL from output
        CONFLUENCE_URL="${{ inputs.confluence_url }}/wiki$(echo "$OUTPUT" | grep -oP 'URL: \K.*' | head -1)"

        if [ -n "$CONFLUENCE_URL" ]; then
          echo "confluence_url=$CONFLUENCE_URL" >> $GITHUB_OUTPUT
          echo "‚úÖ Results uploaded to Confluence: $CONFLUENCE_URL"
        else
          echo "confluence_url=" >> $GITHUB_OUTPUT
          echo "‚úÖ Results uploaded to Confluence"
        fi

    - name: Generate summary
      shell: bash
      run: |
        cd ${{ github.action_path }}
        SCORE="${{ steps.extract_score.outputs.score }}"
        THRESHOLD="${{ inputs.minimum_score }}"
        PASSED="${{ steps.check_score.outputs.passed }}"
        RESULT_FILE="${{ steps.extract_paths.outputs.result_file }}"
        CONFLUENCE_URL="${{ steps.upload_confluence.outputs.confluence_url }}"

        echo "## üìä Unit Test Quality Analysis Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**JIRA Ticket:** ${{ inputs.jira_ticket_id }}" >> $GITHUB_STEP_SUMMARY
        echo "**Test Quality Score:** ${SCORE}/10" >> $GITHUB_STEP_SUMMARY
        echo "**Minimum Threshold:** ${THRESHOLD}/10" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "$PASSED" = "true" ]; then
          echo "‚úÖ **Status:** PASSED - Score meets minimum threshold" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ö†Ô∏è  **Status:** BELOW THRESHOLD - Additional test coverage needed" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üîó Links" >> $GITHUB_STEP_SUMMARY

        # Add PR link if available
        if [ -n "${{ github.event.pull_request.html_url }}" ]; then
          echo "- **Pull Request:** [${{ github.event.pull_request.number }}](${{ github.event.pull_request.html_url }})" >> $GITHUB_STEP_SUMMARY
        fi

        # Add Confluence link if upload was successful
        if [ -n "$CONFLUENCE_URL" ]; then
          echo "- **Confluence Report:** [View Analysis Report](${CONFLUENCE_URL})" >> $GITHUB_STEP_SUMMARY
        fi

        # Add GitHub Actions artifacts link
        echo "- **GitHub Artifacts:** [Download Reports](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### üìÅ Generated Files" >> $GITHUB_STEP_SUMMARY
        echo "- Requirements: \`${{ steps.extract_paths.outputs.requirements_file }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Analysis Report: \`$RESULT_FILE\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract score breakdown from result file
        echo "### üìà Score Breakdown" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        grep -A 1 "Test Coverage Analysis\|Test Quality Assessment\|Edge Cases & Error Scenarios\|Test Assertions Quality\|Code Organization & Maintainability" "$RESULT_FILE" | grep -E "\([0-9.]+/[0-9.]+ points\)" | head -5 >> $GITHUB_STEP_SUMMARY || echo "Score breakdown not available" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "üìÑ **Full report available in job artifacts**" >> $GITHUB_STEP_SUMMARY

    # - name: Fail if score below threshold
    #   if: steps.check_score.outputs.passed == 'false'
    #   shell: bash
    #   run: |
    #     echo "‚ùå Test quality score (${{ steps.extract_score.outputs.score }}) is below minimum threshold (${{ inputs.minimum_score }})"
    #     echo "Please improve test coverage before merging."
    #     exit 1
